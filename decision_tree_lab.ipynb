{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting course evaluations with Decision Trees\n",
    "\n",
    "\n",
    "## 1. CART Algorithm Implementation\n",
    "\n",
    "### 1.1. Tree data structure and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, col=-1, value=None, results=None, tb=None, fb=None):\n",
    "        self.col = col # attribute on which to split\n",
    "        self.value = value # value on which to split\n",
    "        self.results = results # If the node has no children - we store here class labels with their counts\n",
    "        self.tb = tb  # True branch\n",
    "        self.fb = fb  # False branch\n",
    "        \n",
    "def split(rows, column, value):\n",
    "    # define split function according to the value type\n",
    "    split_function = None\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        split_function = lambda row: row[column] >= value\n",
    "    else:\n",
    "        split_function = lambda row: row[column] == value\n",
    "\n",
    "    # Divide the rows into two sets and return them\n",
    "    set1 = [row for row in rows if split_function(row)]\n",
    "    set2 = [row for row in rows if not split_function(row)]\n",
    "    return (set1, set2)\n",
    "\n",
    "def count_labels(rows):\n",
    "    label_count = {}\n",
    "    for row in rows:\n",
    "        # The class label is in the last column\n",
    "        label = row[- 1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Node purity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def gini_impurity(rows):\n",
    "    total = len(rows)\n",
    "    counts = count_labels(rows)\n",
    "    gini = 0\n",
    "    for key, val in counts.items():\n",
    "        p = val / total\n",
    "        gini += p*p\n",
    "        \n",
    "    return (1 - gini)\n",
    "\n",
    "def entropy(rows):\n",
    "    total = len(rows)\n",
    "    counts = count_labels(rows)\n",
    "    ent = 0.0\n",
    "    for key,val in counts.items():\n",
    "        p = val / total\n",
    "        ent = ent - p * log(p, 2)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def variance(rows):\n",
    "    if len(rows) == 0: return 0\n",
    "    num_label = [float(row[- 1]) for row in rows]\n",
    "    mean = sum(num_label) / len(num_label)\n",
    "    variance = sum([(d - mean) ** 2 for d in num_label]) / len(num_label)\n",
    "    return variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tree induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildtree(rows, score_func=entropy, min_improvement=0, min_samples=0, max_depth=None, depth=0):\n",
    "    if len(rows) == 0:\n",
    "        return DecisionNode()\n",
    "    # Compute overall score for the entire rows dataset\n",
    "    current_score = score_func(rows)\n",
    "\n",
    "    # Set up accumulator variables to track the best split criteria\n",
    "    best_score = current_score\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    \n",
    "    # Total number of features - except the last column where we store the class (target)\n",
    "    column_count = len(rows[0]) - 1 \n",
    "    for col in range(0, column_count):\n",
    "        # Generate the list of unique values in\n",
    "        # this column to split on them\n",
    "        column_values = set()\n",
    "        for row in rows:\n",
    "            column_values.add(row[col])\n",
    "            \n",
    "        # Now try splitting the rows \n",
    "        # on each unique value in this column\n",
    "        for value in column_values:\n",
    "            (set1, set2) = split(rows, col, value)\n",
    "\n",
    "            # Evaluate the quality of the split\n",
    "            # p is the proportion of subset set1 \n",
    "            p = float(len(set1)) / len(rows)\n",
    "            split_score = p * score_func(set1) + (1-p) * score_func(set2)\n",
    "            \n",
    "            if split_score < best_score and \\\n",
    "                (len(set1) > min_samples and len(set2) > min_samples) and \\\n",
    "                (current_score - split_score) > min_improvement:\n",
    "                best_score = split_score\n",
    "                best_criteria = (col, value)\n",
    "                best_sets = (set1, set2)\n",
    "\n",
    "    # Create the sub branches\n",
    "    if (current_score - best_score) > min_improvement and \\\n",
    "        (max_depth is None or depth < max_depth) :\n",
    "        # print(\"Splitting on\",best_criteria, \" 2 sets:\", len(best_sets[0]),len(best_sets[1]))\n",
    "        true_branch = buildtree(best_sets[0], score_func, min_improvement, min_samples, max_depth, depth+1)\n",
    "        false_branch = buildtree(best_sets[1], score_func, min_improvement, min_samples, max_depth, depth+1)\n",
    "        return DecisionNode(col=best_criteria[0], value=best_criteria[1],\n",
    "                            tb=true_branch, fb=false_branch)\n",
    "    else: # Done splitting - summarize class labels in leaf nodes\n",
    "        return DecisionNode(results=count_labels(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Tree printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(leaf_labels):\n",
    "    total = 0\n",
    "    result = {}\n",
    "    for label, count in leaf_labels.items():\n",
    "        total += count\n",
    "        result[label] = count\n",
    "\n",
    "    for label, val in result.items():\n",
    "        result[label] = str(int(result[label]/total * 100))+\"%\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, current_branch, attributes=None,  indent='', leaf_funct=prediction):\n",
    "    # Is this a leaf node?\n",
    "    if tree.results != None:\n",
    "        print(indent + current_branch + str(leaf_funct(tree.results)))\n",
    "    else:\n",
    "        # Print the split question\n",
    "        split_col = str(tree.col)\n",
    "        if attributes is not None:\n",
    "            split_col = attributes[tree.col]\n",
    "        split_val = str(tree.value)\n",
    "        if type(tree.value) == int or type(tree.value) == float:\n",
    "            split_val = \">=\" + str(tree.value)\n",
    "        print(indent + current_branch + split_col + ': ' + split_val + '? ')\n",
    "\n",
    "        # Print the branches\n",
    "        indent = indent + '  '\n",
    "        print_tree(tree.tb, 'T->', attributes, indent)\n",
    "        print_tree(tree.fb, 'F->', attributes, indent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(observation, tree):\n",
    "    if tree.results != None:\n",
    "        return prediction(tree.results)\n",
    "    else:\n",
    "        v = observation[tree.col]\n",
    "        branch = None\n",
    "        if isinstance(v, int) or isinstance(v, float):\n",
    "            if v >= tree.value:\n",
    "                branch = tree.tb\n",
    "            else:\n",
    "                branch = tree.fb\n",
    "        else:\n",
    "            if v == tree.value:\n",
    "                branch = tree.tb\n",
    "            else:\n",
    "                branch = tree.fb\n",
    "        return classify(observation, branch)\n",
    "\n",
    "\n",
    "# Classify an observation with missing data\n",
    "def mdclassify(observation, tree):\n",
    "    if tree.results != None:\n",
    "        return prediction(tree.results)\n",
    "    else:\n",
    "        v = observation[tree.col]\n",
    "        if v == None:\n",
    "            tr, fr = mdclassify(observation, tree.tb), mdclassify(observation, tree.fb)\n",
    "            tcount = sum(tr.values())\n",
    "            fcount = sum(fr.values())\n",
    "            tw = float(tcount) / (tcount + fcount)\n",
    "            fw = float(fcount) / (tcount + fcount)\n",
    "            result = {}\n",
    "            for k, v in tr.items(): result[k] = v * tw\n",
    "            for k, v in fr.items(): result[k] = v * fw\n",
    "            return result\n",
    "        else:\n",
    "            if isinstance(v, int) or isinstance(v, float):\n",
    "                if v >= tree.value:\n",
    "                    branch = tree.tb\n",
    "                else:\n",
    "                    branch = tree.fb\n",
    "            else:\n",
    "                if v == tree.value:\n",
    "                    branch = tree.tb\n",
    "                else:\n",
    "                    branch = tree.fb\n",
    "            return mdclassify(observation, branch)\n",
    "\n",
    "def max_depth(tree):\n",
    "    if tree.results != None:\n",
    "        return 0\n",
    "    else:\n",
    "        # Compute the depth of each subtree\n",
    "        tDepth = max_depth(tree.tb)\n",
    "        fDepth = max_depth(tree.fb)\n",
    "\n",
    "        # Use the larger one\n",
    "        if (tDepth > fDepth):\n",
    "            return tDepth + 1\n",
    "        else:\n",
    "            return fDepth + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset to numeric\n",
    "\n",
    "The dataset for this lab contains about 450 anonymized student evaluations collected at the University of Texas at Austin, and used in the following publication: __\"Beauty in the Classroom: Instructors' Pulchritude and Putative Pedagogical Productivity\"__. You can learn how the data was collected and the meaning of various data attributes following this [link](https://chance.amstat.org/2013/04/looking-good/).\n",
    "\n",
    "We use a subset of attributes: 'rank', 'ethnicity', 'gender', 'language', 'age', 'class_size', 'class_level',  'avg_beauty_score',  and 'professor_evaluation_score' and try to predict course evaluation score. \n",
    "\n",
    "This smaller dataset can be downloaded from [here](https://drive.google.com/file/d/18wV59AYCVCqL1BIDBYLpL73emp7h8d0T/view?usp=sharing). Download the dataset into your data directory, and update the path to your file in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data_sets/StudentEvaluations.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset into a data frame _df_. First, we will make all attributes numeric, and then we will convert all attributes to categorical labels. The _df_ points to the original dataset. We are going to create two copies: _df_num_ and _df_cat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that not all attributes are numeric. Let's make them all numeric by using the pandas _replace_ method (see [this post](https://www.geeksforgeeks.org/how-to-convert-categorical-variable-to-numeric-in-pandas/)), which replaces the original values with the predefined set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a copy of the original data frame\n",
    "df_num = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace 'rank', 'ethnicity', 'gender', 'language', and 'cls_level' columns.\n",
    "Let's find out what are the values in each column - to know what to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank\n",
    "df_num['rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethnicity\n",
    "df_num['ethnicity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender\n",
    "df_num['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language\n",
    "df_num['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class level\n",
    "df_num['cls_level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning a numeric code to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num['rank'].replace(['tenure track', 'tenured', 'teaching'], [1, 2, 3], inplace=True)\n",
    "df_num['ethnicity'].replace(['minority', 'not minority'], [1, 0], inplace=True)\n",
    "df_num['gender'].replace(['female', 'male'], [1, 2], inplace=True)\n",
    "df_num['language'].replace(['english', 'non-english'], [1, 0], inplace=True)\n",
    "df_num['cls_level'].replace(['upper', 'lower'], [2,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the class label an integer by rounding course evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num['course_eval'] = df_num['course_eval'].round(0).astype(int)\n",
    "\n",
    "df_num['course_eval'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check what it looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore if there is any correlation between course score and any other attributes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "corr = df_num.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree: numeric dataset\n",
    "### 3.1. Using custom ID3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_rows = df_num.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = df_num.columns.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be a regression tree, as we are trying to predict a numeric score. We have to use __variance__ as a measure of node purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildtree(data_rows, score_func=variance, min_improvement=0, min_samples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree, '', columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Using sklearn library\n",
    "The decision tree algorithm in sklearn library is not implemented very well. It requires all the attributes to be numeric - while decision trees work best with the categorical attributes. Nevertheless, we will try to run it and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class label has to be binary. So we are going to replace the 'course_eval' column with 3 bins, corresponding to the score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide dataset into features and class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_num.loc[:, df_num.columns != 'course_eval']\n",
    "print(X.columns)\n",
    "\n",
    "Y = df_num.loc[:, df_num.columns == 'course_eval']\n",
    "print(Y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset intro training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different parameters can be specified to build the model:\n",
    "\n",
    "`model = tree.DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        max_depth=None, \n",
    "        min_samples_split=2, \n",
    "        min_samples_leaf=1, \n",
    "        max_features=None, \n",
    "        random_state=None, \n",
    "        min_density=None, \n",
    "        compute_importances=None, \n",
    "        max_leaf_nodes=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 7)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_list = df_num.columns.to_numpy().tolist()\n",
    "from sklearn.tree import export_text\n",
    "r = export_text(model, feature_names=columns_list[:-1])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Categorical dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replace all the numeric columns in _df_ by categorical labels, using [Pandas _cut_ method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html).\n",
    "\n",
    "First we create a copy of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cat.columns)\n",
    "print(df_cat[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to bin columns 'age', 'cls_students', 'bty_avg', 'prof_eval', and 'course_eval'. Everything else is already categorical.\n",
    "\n",
    "First thing is to determine the domain for each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "age_column = df_cat[\"age\"]\n",
    "print(\"From:\", age_column.min(), \"to\", age_column.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty score\n",
    "bty_ave_column = df_cat[\"bty_avg\"]\n",
    "print(\"From:\", bty_ave_column.min(), \"to\", bty_ave_column.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class size\n",
    "num_students_column = df_cat[\"cls_students\"]\n",
    "print(\"From:\", num_students_column.min(), \"to\", num_students_column.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professor score\n",
    "prof_eval_column = df_cat[\"prof_eval\"]\n",
    "print(\"From:\", prof_eval_column.min(), \"to\", prof_eval_column.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course score (class label)\n",
    "course_eval_column = df_cat[\"course_eval\"]\n",
    "print(\"From:\", course_eval_column.min(), \"to\", course_eval_column.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Age to bins\n",
    "bins = [20, 35, 50, 65, np.inf]\n",
    "names = [ '20-35', '35-50',  '50-65', '65+']\n",
    "\n",
    "df_cat['age'] = pd.cut(df_cat['age'], bins, labels=names)\n",
    "\n",
    "# Class size to bins\n",
    "bins = [0, 15, 30, 50, 100, np.inf]\n",
    "names = [ '<15', '15-30',  '30-50', '50-100', '100+']\n",
    "\n",
    "df_cat['cls_students'] = pd.cut(df_cat['cls_students'], bins, labels=names)\n",
    "\n",
    "# Beauty average to bins\n",
    "bins = [0, 3, 6, 9, np.inf]\n",
    "names = [ '<3', '3-6',  '6-9', '9+']\n",
    "\n",
    "df_cat['bty_avg'] = pd.cut(df_cat['bty_avg'], bins, labels=names)\n",
    "\n",
    "# Professor score to bins\n",
    "bins = [0, 2, 3, 4, 4.5, np.inf]\n",
    "names = [ '<2', '2-3',  '3-4', '4-4.5', '4.5+']\n",
    "\n",
    "df_cat['prof_eval'] = pd.cut(df_cat['prof_eval'], bins, labels=names)\n",
    "\n",
    "# Course score to class label\n",
    "bins = [0, 2, 3, 4, 4.5, np.inf]\n",
    "names = [ 'bad', 'fair',  'average', 'good', 'excellent']\n",
    "\n",
    "df_cat['course_eval'] = pd.cut(df_cat['course_eval'], bins, labels=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cat.columns)\n",
    "print(df_cat[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Tree: categorical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_rows = df_cat.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = df_cat.columns.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is a decision tree (classification), we use entropy as a measure of leaf purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildtree(data_rows, score_func=entropy, min_improvement=0, min_samples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree, '', columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best we could do for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. [TASK 1] Using learned tree for classification\n",
    "\n",
    "Recall at least 3 different courses you have taken, and record the values of the attributes, and the course score that you have given to the course. Run the predict function to see if the decision tree correctly predicts your score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. [TASK 2] Important factors\n",
    "What are the most important attributes in separating good course evaluations from the bad ones? Support your answer by analyzing the tree levels and the leaf outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. [TASK 3] Rules\n",
    "Extract from the tree at least 3 rules that you find most reliable. Did any of these rules surprise you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. [BONUS] Web visualization\n",
    "The visualizations of the tree can be made much more expressive with the _d3.js_ visualization library. Sample  _public_html_ folder containing all the required html, css, and javascript files, including the library itself, is provided. \n",
    "\n",
    "In order to create this visualization, all you need is a json file which represents the nodes of the final decision tree. Learn the required JSON format, and implement this conversion in a separate file __dtree_to_json.py__. Submit your file with the lab. \n",
    "\n",
    "Copy the resulting json file into the data directory in the _public_html_ folder, and update the name of the json file in the JavaScript code on line 39 in _index.html_. \n",
    "\n",
    "Note that Chrome would not allow you to run the visualization locally - so if you want to present it to the world you should use a web server. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
